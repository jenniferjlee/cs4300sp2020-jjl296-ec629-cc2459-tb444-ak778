{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1076)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('app/irsystem/data/final_data_url_polarity_filtered.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>transcript</th>\n",
       "      <th>source</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-Year-Old Has Been Using His Flying Lessons ...</td>\n",
       "      <td>https://www.goodnewsnetwork.org/teen-flies-sup...</td>\n",
       "      <td>2020-04-09</td>\n",
       "      <td>16-year-old TJ Kim may not have his driver’s l...</td>\n",
       "      <td>goodnewsnetwork.org</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.214592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tyler Perry Picks Up the Tab for All Groceries...</td>\n",
       "      <td>https://www.goodnewsnetwork.org/tyler-perry-pa...</td>\n",
       "      <td>2020-04-09</td>\n",
       "      <td>It’s not uncommon for movie mogul Tyler Perry ...</td>\n",
       "      <td>goodnewsnetwork.org</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.234015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  16-Year-Old Has Been Using His Flying Lessons ...   \n",
       "1  Tyler Perry Picks Up the Tab for All Groceries...   \n",
       "\n",
       "                                                 url       date  \\\n",
       "0  https://www.goodnewsnetwork.org/teen-flies-sup... 2020-04-09   \n",
       "1  https://www.goodnewsnetwork.org/tyler-perry-pa... 2020-04-09   \n",
       "\n",
       "                                          transcript               source  \\\n",
       "0  16-year-old TJ Kim may not have his driver’s l...  goodnewsnetwork.org   \n",
       "1  It’s not uncommon for movie mogul Tyler Perry ...  goodnewsnetwork.org   \n",
       "\n",
       "  subtitle  score  comments thumbnail  polarity  \n",
       "0     None    NaN       NaN      None  0.214592  \n",
       "1     None    NaN       NaN      None  0.234015  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = list(df['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def tokenize_lemmatize(d):\n",
    "    final_tokens = []\n",
    "    for token in simple_preprocess(d) :\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            final_tokens.append(stemmer.stem(WordNetLemmatizer().lemmatize(token, pos='v')))\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_transcripts = []\n",
    "for t in transcripts:\n",
    "    processed_transcripts.append(tokenize_lemmatize(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below = 10, no_above= 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = [dictionary.doc2bow(t) for t in processed_transcripts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow, num_topics = 6, id2word = dictionary, passes = 12, workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 :\n",
      "Words : 0.022*\"say\" + 0.011*\"year\" + 0.008*\"time\" + 0.007*\"tell\" + 0.007*\"like\" + 0.007*\"know\" + 0.007*\"go\" + 0.006*\"peopl\" + 0.006*\"help\" + 0.006*\"famili\" \n",
      "\n",
      "Topic 2 :\n",
      "Words : 0.012*\"say\" + 0.010*\"year\" + 0.007*\"world\" + 0.006*\"energi\" + 0.006*\"power\" + 0.006*\"climat\" + 0.005*\"chang\" + 0.005*\"tree\" + 0.005*\"plant\" + 0.005*\"peopl\" \n",
      "\n",
      "Topic 3 :\n",
      "Words : 0.008*\"say\" + 0.008*\"essenti\" + 0.007*\"skin\" + 0.006*\"plastic\" + 0.006*\"research\" + 0.005*\"help\" + 0.005*\"oil\" + 0.005*\"patient\" + 0.005*\"pain\" + 0.005*\"effect\" \n",
      "\n",
      "Topic 4 :\n",
      "Words : 0.010*\"say\" + 0.007*\"church\" + 0.006*\"https\" + 0.006*\"peopl\" + 0.006*\"best\" + 0.005*\"http\" + 0.005*\"year\" + 0.005*\"support\" + 0.004*\"experi\" + 0.004*\"browser\" \n",
      "\n",
      "Topic 5 :\n",
      "Words : 0.017*\"market\" + 0.011*\"product\" + 0.011*\"compani\" + 0.008*\"busi\" + 0.007*\"report\" + 0.006*\"global\" + 0.006*\"servic\" + 0.006*\"industri\" + 0.005*\"share\" + 0.005*\"custom\" \n",
      "\n",
      "Topic 6 :\n",
      "Words : 0.019*\"say\" + 0.014*\"peopl\" + 0.008*\"year\" + 0.007*\"work\" + 0.005*\"world\" + 0.005*\"citi\" + 0.005*\"time\" + 0.004*\"communiti\" + 0.004*\"countri\" + 0.004*\"come\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic', str(idx + 1), ':\\nWords :', topic, '\\n') \n",
    "#     print(\"Topic: {} \\nWords: {}\".format(idx + 1, topic ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
